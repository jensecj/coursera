#+OPTIONS: toc:nil html-postamble:nil
#+PROPERTY: header-args:python :session week-2-sess-1 :tangle optimization.py :exports code

* Optimization Methods
Up till now we have only looked at gradient descent, but there are more methods
for optimization. Here will look at improving the gradient descent method we
have already used, and looking at some other popular approaches.

** setup
#+begin_src python :results silent
import numpy as np
import matplotlib.pyplot as plt
import scipy.io
import math
import sklearn
import sklearn.datasets

from opt_utils import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation
from opt_utils import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset
from testCases import *

plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'
#+end_src

** gradient descent
In the approach we have used thus far, we have updated the parameters using the
following rule:

for $l = 1,...,L$:
$\Theta^{[l]} = \Theta^{[l]} - \alpha \nabla_{\Theta}^{[l]}$.

That is, we update each parameter by nudging it by the /learning rate/ with
respect to the gradient of the parameter.

#+begin_src python :results silent
def update_parameters_with_gd(parameters, grads, learning_rate):
    """
    Update parameters using one step of gradient descent
    
    Arguments:
    parameters -- python dictionary containing your parameters to be updated:
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    grads -- python dictionary containing your gradients to update each parameters:
                    grads['dW' + str(l)] = dWl
                    grads['db' + str(l)] = dbl
    learning_rate -- the learning rate, scalar.
    
    Returns:
    parameters -- python dictionary containing your updated parameters 
    """
    L = len(parameters) // 2 # number of layers in the neural networks
    
    # Update rule for each parameter
    for l in range(L):
        parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate * grads["dW" + str(l+1)]
        parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * grads["db" + str(l+1)]
        
    return parameters
#+end_src

#+begin_src python :results output :exports both
parameters, grads, learning_rate = update_parameters_with_gd_test_case()

parameters = update_parameters_with_gd(parameters, grads, learning_rate)
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
#+end_src

#+RESULTS:
#+begin_example
W1 = [[ 1.63535156 -0.62320365 -0.53718766]
 [-1.07799357  0.85639907 -2.29470142]]
b1 = [[ 1.74604067]
 [-0.75184921]]
W2 = [[ 0.32171798 -0.25467393  1.46902454]
 [-2.05617317 -0.31554548 -0.3756023 ]
 [ 1.1404819  -1.09976462 -0.1612551 ]]
b2 = [[-0.88020257]
 [ 0.02561572]
 [ 0.57539477]]
#+end_example

#+begin_example
Expected Output:

W1	[[ 1.63535156 -0.62320365 -0.53718766] [-1.07799357 0.85639907 -2.29470142]]
b1	[[ 1.74604067] [-0.75184921]]
W2	[[ 0.32171798 -0.25467393 1.46902454] [-2.05617317 -0.31554548 -0.3756023 ] [ 1.1404819 -1.09976462 -0.1612551 ]]
b2	[[-0.88020257] [ 0.02561572] [ 0.57539477]]
#+end_example

This approach is also known as *Batch gradient descent* (BGD), because we look at all
the example at once, in a single batch.

This is a special case of the *mini-batch gradient descent* (MBGD) method, where the
mini-batch size is the total number of examples we have.

Another special case is if we pick the mini-batch size to be 1, e.g. we learn by
looking at a single example at a time. This approach is known as *stochastic
gradient descent* (SGD)

We can illustrate the differences these two 'extreme' approaches have on our learning:

[[file:images/kiank_sgd.png]]

We can see that gradient descent takes a very straight direction towards the
goal, while SGD oscillates a lot before reaching convergence.

In practice the best approach to use is something in between these two
extremes. The reason begin that with very big datasets, it is simply too
expensive to look at all the examples at once, and it takes too long to reach
convergence by looking at only a single example at a time.

[[file:images/kiank_minibatch.png]]

** mini-batch gradient descent
To use mini-batch gradient descent. we first need to create our mini-batches
from the training dataset.

this requires two steps
- shuffle the training dataset and labels the same way
- partition the training dataset and labels into mini-batch sized chunks

The last mini-batch may not be entirely full, which is fine. It is also
important to remember that we need to keep training examples together with their
correct labels, so we should shuffle them together.

[[file:images/kiank_shuffle.png]]

[[file:images/kiank_partition.png]]

Mini-batch sizes are usually a power of 2, e.g. 16,32,64,128,..., with the size
depending on what works for the particular project.

For partitioning mini-batches, we know that there will be
$\lfloor \frac{m}{\text{mini-batch-size}} \rfloor$ full mini-batches, and the last mini-batch
will contain $m - \text{mini-batch-size} \times \lfloor
\frac{m}{\text{mini-batch-size}} \rfloor$ elements.

#+begin_src python :results silent
def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):
    """
    Creates a list of random minibatches from (X, Y)
    
    Arguments:
    X -- input data, of shape (input size, number of examples)
    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)
    mini_batch_size -- size of the mini-batches, integer
    
    Returns:
    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)
    """
    np.random.seed(seed)            # To make your "random" minibatches the same as ours
    m = X.shape[1]                  # number of training examples
    mini_batches = []
    
    # Step 1: Shuffle (X, Y)
    permutation = list(np.random.permutation(m))
    shuffled_X = X[:, permutation]
    shuffled_Y = Y[:, permutation].reshape((1,m))
    
    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.
    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning
    for k in range(0, num_complete_minibatches):
        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size]
        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1) * mini_batch_size]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
        
    # Handling the end case (last mini-batch < mini_batch_size)
    if m % mini_batch_size != 0:
        remaining = m - mini_batch_size * math.floor(m/mini_batch_size)
        mini_batch_X = shuffled_X[:, -remaining:]
        mini_batch_Y = shuffled_Y[:, -remaining:]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
        
    return mini_batches
#+end_src

#+begin_src python :results output :exports both
X_assess, Y_assess, mini_batch_size = random_mini_batches_test_case()
mini_batches = random_mini_batches(X_assess, Y_assess, mini_batch_size)

print ("shape of the 1st mini_batch_X: " + str(mini_batches[0][0].shape))
print ("shape of the 2nd mini_batch_X: " + str(mini_batches[1][0].shape))
print ("shape of the 3rd mini_batch_X: " + str(mini_batches[2][0].shape))
print ("shape of the 1st mini_batch_Y: " + str(mini_batches[0][1].shape))
print ("shape of the 2nd mini_batch_Y: " + str(mini_batches[1][1].shape)) 
print ("shape of the 3rd mini_batch_Y: " + str(mini_batches[2][1].shape))
print ("mini batch sanity check: " + str(mini_batches[0][0][0][0:3]))
#+end_src

#+RESULTS:
: shape of the 1st mini_batch_X: (12288, 64)
: shape of the 2nd mini_batch_X: (12288, 64)
: shape of the 3rd mini_batch_X: (12288, 20)
: shape of the 1st mini_batch_Y: (1, 64)
: shape of the 2nd mini_batch_Y: (1, 64)
: shape of the 3rd mini_batch_Y: (1, 20)
: mini batch sanity check: [ 0.90085595 -0.7612069   0.2344157 ]

#+begin_example
Expected Output:

shape of the 1st mini_batch_X	(12288, 64)
shape of the 2nd mini_batch_X	(12288, 64)
shape of the 3rd mini_batch_X	(12288, 20)
shape of the 1st mini_batch_Y	(1, 64)
shape of the 2nd mini_batch_Y	(1, 64)
shape of the 3rd mini_batch_Y	(1, 20)
mini batch sanity check	[ 0.90085595 -0.7612069 0.2344157 ]
#+end_example

** momentum
The reason for the oscillation in MBGD is because it makes directional changes
based on only a subset of the entire training set, so there is some variance
between the minibatches, because the examples differ.

Momentum is a technique to 'smooth' these oscillations.

The idea is to take into account the previous gradients, and base our updates on
those as well. We keep track of the previous velocities of the gradients using
an exponentially weighted moving average.

[[file:images/opt_momentum.png]]

First, we need to initialize the velocities for the parameters.

#+begin_src python :results silent
def initialize_velocity(parameters):
    """
    Initializes the velocity as a python dictionary with:
                - keys: "dW1", "db1", ..., "dWL", "dbL"
                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.
    Arguments:
    parameters -- python dictionary containing your parameters.
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    
    Returns:
    v -- python dictionary containing the current velocity.
                    v['dW' + str(l)] = velocity of dWl
                    v['db' + str(l)] = velocity of dbl
    """
    L = len(parameters) // 2 # number of layers in the neural networks
    v = {}
    
    # Initialize velocity
    for l in range(L):
        v["dW" + str(l+1)] = np.zeros((parameters["W" + str(l+1)].shape))
        v["db" + str(l+1)] = np.zeros((parameters["b" + str(l+1)].shape))
        
    return v
#+end_src

#+begin_src python :results output :exports both
parameters = initialize_velocity_test_case()

v = initialize_velocity(parameters)
print("v[\"dW1\"] = " + str(v["dW1"]))
print("v[\"db1\"] = " + str(v["db1"]))
print("v[\"dW2\"] = " + str(v["dW2"]))
print("v[\"db2\"] = " + str(v["db2"]))
#+end_src

#+RESULTS:
#+begin_example
v["dW1"] = [[0. 0. 0.]
 [0. 0. 0.]]
v["db1"] = [[0.]
 [0.]]
v["dW2"] = [[0. 0. 0.]
 [0. 0. 0.]
 [0. 0. 0.]]
v["db2"] = [[0.]
 [0.]
 [0.]]
#+end_example

#+begin_example
Expected Output:

v["dW1"]	[[ 0. 0. 0.] [ 0. 0. 0.]]
v["db1"]	[[ 0.] [ 0.]]
v["dW2"]	[[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]]
v["db2"]	[[ 0.] [ 0.] [ 0.]]
#+end_example

Now we can add the momentum to our parameters when updating.

Momentum is updates using $v_{\Theta} = \beta v_{\Theta} + (1-\beta) \Theta$ for
each parameter $\Theta$, on each layer. $\beta$ is the hyperparameter which
determines the momentum of the velocity we have collected. We can see that, the
higher we pick $\beta$, the more importance we give to the momentum, while
dampening the parameter. We can also see that if we pick $\beta = 0$, we
completely ignore the velocity, and this becomes standard gradient descent.

#+begin_src python :results silent
def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):
    """
    Update parameters using Momentum
    
    Arguments:
    parameters -- python dictionary containing your parameters:
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    grads -- python dictionary containing your gradients for each parameters:
                    grads['dW' + str(l)] = dWl
                    grads['db' + str(l)] = dbl
    v -- python dictionary containing the current velocity:
                    v['dW' + str(l)] = ...
                    v['db' + str(l)] = ...
    beta -- the momentum hyperparameter, scalar
    learning_rate -- the learning rate, scalar
    
    Returns:
    parameters -- python dictionary containing your updated parameters
    v -- python dictionary containing your updated velocities
    """
    L = len(parameters) // 2 # number of layers in the neural networks
    
    # Momentum update for each parameter
    for l in range(L):
        # compute velocities
        v["dW" + str(l+1)] = beta * v["dW" + str(l+1)] + (1-beta)*grads["dW" + str(l+1)]
        v["db" + str(l+1)] = beta * v["db" + str(l+1)] + (1-beta)*grads["db" + str(l+1)]
        # update parameters
        parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate * v["dW" + str(l+1)]
        parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * v["db" + str(l+1)]
        
    return parameters, v
#+end_src

#+begin_src python :results output :exports both
parameters, grads, v = update_parameters_with_momentum_test_case()

parameters, v = update_parameters_with_momentum(parameters, grads, v, beta = 0.9, learning_rate = 0.01)
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
print("v[\"dW1\"] = " + str(v["dW1"]))
print("v[\"db1\"] = " + str(v["db1"]))
print("v[\"dW2\"] = " + str(v["dW2"]))
print("v[\"db2\"] = " + str(v["db2"]))
#+end_src

#+RESULTS:
#+begin_example
W1 = [[ 1.62544598 -0.61290114 -0.52907334]
 [-1.07347112  0.86450677 -2.30085497]]
b1 = [[ 1.74493465]
 [-0.76027113]]
W2 = [[ 0.31930698 -0.24990073  1.4627996 ]
 [-2.05974396 -0.32173003 -0.38320915]
 [ 1.13444069 -1.0998786  -0.1713109 ]]
b2 = [[-0.87809283]
 [ 0.04055394]
 [ 0.58207317]]
v["dW1"] = [[-0.11006192  0.11447237  0.09015907]
 [ 0.05024943  0.09008559 -0.06837279]]
v["db1"] = [[-0.01228902]
 [-0.09357694]]
v["dW2"] = [[-0.02678881  0.05303555 -0.06916608]
 [-0.03967535 -0.06871727 -0.08452056]
 [-0.06712461 -0.00126646 -0.11173103]]
v["db2"] = [[0.02344157]
 [0.16598022]
 [0.07420442]]
#+end_example

#+begin_example
Expected Output:

W1	[[ 1.62544598 -0.61290114 -0.52907334] [-1.07347112 0.86450677 -2.30085497]]
b1	[[ 1.74493465] [-0.76027113]]
W2	[[ 0.31930698 -0.24990073 1.4627996 ] [-2.05974396 -0.32173003 -0.38320915] [ 1.13444069 -1.0998786 -0.1713109 ]]
b2	[[-0.87809283] [ 0.04055394] [ 0.58207317]]
v["dW1"]	[[-0.11006192 0.11447237 0.09015907] [ 0.05024943 0.09008559 -0.06837279]]
v["db1"]	[[-0.01228902] [-0.09357694]]
v["dW2"]	[[-0.02678881 0.05303555 -0.06916608] [-0.03967535 -0.06871727 -0.08452056] [-0.06712461 -0.00126646 -0.11173103]]
v["db2"]	[[ 0.02344157] [ 0.16598022] [ 0.07420442]]
#+end_example

important note: since we initialize the velocity with zeroes, the first few
iterations will be 'incorrect', it takes a little time for the algorithm to
build-up a proper momentum to dampen the gradients, this could be alleviated
using techniques such as bias-correcting.

** adam
Adam is a very popular optimization algorithm, as it has shown to work well on
many types of problems.

It combines techniques from momentum and RMSProp (see notes).

the approach:
- calculate an EWA of past gradients
- calculate an EWA of the squares of past gradients
- update parameters using these two moving averages

You can choose to apply bias-correcting to the moving averages if you care about
the first few iterations, but in practice, this is commonly ignored.

The update rule for Adam, for our common parameters $W$ and $b$:
$$\begin{cases}
v_{\Theta} = \beta_1 v_{\Theta} + (1 - \beta_1) \nabla_{\Theta} \\
v^{corrected}_{\Theta} = \frac{v_{\Theta}}{1 - (\beta_1)^t} \\
s_{\Theta} = \beta_2 s_{\Theta} + (1 - \beta_2) (\nabla_{\Theta})^2 \\
s^{corrected}_{\Theta} = \frac{s_{\Theta}}{1 - (\beta_1)^t} \\
\Theta = \Theta - \alpha \frac{v^{corrected}_{\Theta}}{\sqrt{s^{corrected}_{\Theta}} + \varepsilon}
\end{cases}$$

for each parameter $\Theta$ in each layer.

where
$t$ is the number of steps of Adam,
$L$ is the number of layers,
$\beta1,
\beta2$ are the hyperparameters of the moving averages,
$\alpha$ is the learning rate, and
$\epsilon$ is a small number we use for numerical stability, to avoid
dividing by zero.

#+begin_src python :results silent
def initialize_adam(parameters) :
    """
    Initializes v and s as two python dictionaries with:
                - keys: "dW1", "db1", ..., "dWL", "dbL"
                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.
    
    Arguments:
    parameters -- python dictionary containing your parameters.
                    parameters["W" + str(l)] = Wl
                    parameters["b" + str(l)] = bl
    
    Returns:
    v -- python dictionary that will contain the exponentially weighted average of the gradient.
                    v["dW" + str(l)] = ...
                    v["db" + str(l)] = ...
    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.
                    s["dW" + str(l)] = ...
                    s["db" + str(l)] = ...

    """
    L = len(parameters) // 2 # number of layers in the neural networks
    v = {}
    s = {}
    
    # Initialize v, s. Input: "parameters". Outputs: "v, s".
    for l in range(L):
        v["dW" + str(l+1)] = np.zeros(parameters["W" + str(l+1)].shape)
        v["db" + str(l+1)] = np.zeros(parameters["b" + str(l+1)].shape)
        s["dW" + str(l+1)] = np.zeros(parameters["W" + str(l+1)].shape)
        s["db" + str(l+1)] = np.zeros(parameters["b" + str(l+1)].shape)
        
    return v, s
#+end_src

#+begin_src python :results output :exports both
parameters = initialize_adam_test_case()

v, s = initialize_adam(parameters)
print("v[\"dW1\"] = " + str(v["dW1"]))
print("v[\"db1\"] = " + str(v["db1"]))
print("v[\"dW2\"] = " + str(v["dW2"]))
print("v[\"db2\"] = " + str(v["db2"]))
print("s[\"dW1\"] = " + str(s["dW1"]))
print("s[\"db1\"] = " + str(s["db1"]))
print("s[\"dW2\"] = " + str(s["dW2"]))
print("s[\"db2\"] = " + str(s["db2"]))
#+end_src

#+RESULTS:
#+begin_example
v["dW1"] = [[0. 0. 0.]
 [0. 0. 0.]]
v["db1"] = [[0.]
 [0.]]
v["dW2"] = [[0. 0. 0.]
 [0. 0. 0.]
 [0. 0. 0.]]
v["db2"] = [[0.]
 [0.]
 [0.]]
s["dW1"] = [[0. 0. 0.]
 [0. 0. 0.]]
s["db1"] = [[0.]
 [0.]]
s["dW2"] = [[0. 0. 0.]
 [0. 0. 0.]
 [0. 0. 0.]]
s["db2"] = [[0.]
 [0.]
 [0.]]
#+end_example

#+begin_example
Expected Output:

v["dW1"]	[[ 0. 0. 0.] [ 0. 0. 0.]]
v["db1"]	[[ 0.] [ 0.]]
v["dW2"]	[[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]]
v["db2"]	[[ 0.] [ 0.] [ 0.]]
s["dW1"]	[[ 0. 0. 0.] [ 0. 0. 0.]]
s["db1"]	[[ 0.] [ 0.]]
s["dW2"]	[[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]]
s["db2"]	[[ 0.] [ 0.] [ 0.]]
#+end_example

#+begin_src python :results silent
def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01, beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):
    """
    Update parameters using Adam
    
    Arguments:
    parameters -- python dictionary containing your parameters:
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    grads -- python dictionary containing your gradients for each parameters:
                    grads['dW' + str(l)] = dWl
                    grads['db' + str(l)] = dbl
    v -- Adam variable, moving average of the first gradient, python dictionary
    s -- Adam variable, moving average of the squared gradient, python dictionary
    learning_rate -- the learning rate, scalar.
    beta1 -- Exponential decay hyperparameter for the first moment estimates
    beta2 -- Exponential decay hyperparameter for the second moment estimates
    epsilon -- hyperparameter preventing division by zero in Adam updates

    Returns:
    parameters -- python dictionary containing your updated parameters
    v -- Adam variable, moving average of the first gradient, python dictionary
    s -- Adam variable, moving average of the squared gradient, python dictionary
    """
    L = len(parameters) // 2                 # number of layers in the neural networks
    v_corrected = {}                         # Initializing first moment estimate, python dictionary
    s_corrected = {}                         # Initializing second moment estimate, python dictionary
    
    # Perform Adam update on all parameters
    for l in range(L):
        # Moving average of the gradients. Inputs: "v, grads, beta1". Output: "v".
        v["dW" + str(l+1)] = beta1 * v["dW" + str(l+1)] + (1-beta1) * grads["dW" + str(l+1)]
        v["db" + str(l+1)] = beta1 * v["db" + str(l+1)] + (1-beta1) * grads["db" + str(l+1)]
        
        # Compute bias-corrected first moment estimate. Inputs: "v, beta1, t". Output: "v_corrected".
        v_corrected["dW" + str(l+1)] = v["dW" + str(l+1)] / (1-np.power((beta1), t))
        v_corrected["db" + str(l+1)] = v["db" + str(l+1)] / (1-np.power((beta1), t))
        
        # Moving average of the squared gradients. Inputs: "s, grads, beta2". Output: "s".
        s["dW" + str(l+1)] = beta2 * s["dW" + str(l+1)] + (1-beta2) * np.power(grads["dW" + str(l+1)], 2)
        s["db" + str(l+1)] = beta2 * s["db" + str(l+1)] + (1-beta2) * np.power(grads["db" + str(l+1)], 2)
        
        # Compute bias-corrected second raw moment estimate. Inputs: "s, beta2, t". Output: "s_corrected".
        s_corrected["dW" + str(l+1)] = s["dW" + str(l+1)] / (1-np.power((beta2), t))
        s_corrected["db" + str(l+1)] = s["db" + str(l+1)] / (1-np.power((beta2), t))
        
        # Update parameters. Inputs: "parameters, learning_rate, v_corrected, s_corrected, epsilon". Output: "parameters".
        parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate * (v_corrected["dW" + str(l+1)] / (np.sqrt(s_corrected["dW" + str(l+1)]) + epsilon))
        parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * (v_corrected["db" + str(l+1)] / (np.sqrt(s_corrected["db" + str(l+1)]) + epsilon))
        
    return parameters, v, s
#+end_src

#+begin_src python :results output :exports both
parameters, grads, v, s = update_parameters_with_adam_test_case()
parameters, v, s  = update_parameters_with_adam(parameters, grads, v, s, t = 2)

print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
print("v[\"dW1\"] = " + str(v["dW1"]))
print("v[\"db1\"] = " + str(v["db1"]))
print("v[\"dW2\"] = " + str(v["dW2"]))
print("v[\"db2\"] = " + str(v["db2"]))
print("s[\"dW1\"] = " + str(s["dW1"]))
print("s[\"db1\"] = " + str(s["db1"]))
print("s[\"dW2\"] = " + str(s["dW2"]))
print("s[\"db2\"] = " + str(s["db2"]))
#+end_src

#+RESULTS:
#+begin_example
W1 = [[ 1.63178673 -0.61919778 -0.53561312]
 [-1.08040999  0.85796626 -2.29409733]]
b1 = [[ 1.75225313]
 [-0.75376553]]
W2 = [[ 0.32648046 -0.25681174  1.46954931]
 [-2.05269934 -0.31497584 -0.37661299]
 [ 1.14121081 -1.09244991 -0.16498684]]
b2 = [[-0.88529979]
 [ 0.03477238]
 [ 0.57537385]]
v["dW1"] = [[-0.11006192  0.11447237  0.09015907]
 [ 0.05024943  0.09008559 -0.06837279]]
v["db1"] = [[-0.01228902]
 [-0.09357694]]
v["dW2"] = [[-0.02678881  0.05303555 -0.06916608]
 [-0.03967535 -0.06871727 -0.08452056]
 [-0.06712461 -0.00126646 -0.11173103]]
v["db2"] = [[0.02344157]
 [0.16598022]
 [0.07420442]]
s["dW1"] = [[0.00121136 0.00131039 0.00081287]
 [0.0002525  0.00081154 0.00046748]]
s["db1"] = [[1.51020075e-05]
 [8.75664434e-04]]
s["dW2"] = [[7.17640232e-05 2.81276921e-04 4.78394595e-04]
 [1.57413361e-04 4.72206320e-04 7.14372576e-04]
 [4.50571368e-04 1.60392066e-07 1.24838242e-03]]
s["db2"] = [[5.49507194e-05]
 [2.75494327e-03]
 [5.50629536e-04]]
#+end_example

#+begin_example
Expected Output:

W1	[[ 1.63178673 -0.61919778 -0.53561312] [-1.08040999 0.85796626 -2.29409733]]
b1	[[ 1.75225313] [-0.75376553]]
W2	[[ 0.32648046 -0.25681174 1.46954931] [-2.05269934 -0.31497584 -0.37661299] [ 1.14121081 -1.09245036 -0.16498684]]
b2	[[-0.88529978] [ 0.03477238] [ 0.57537385]]
v["dW1"]	[[-0.11006192 0.11447237 0.09015907] [ 0.05024943 0.09008559 -0.06837279]]
v["db1"]	[[-0.01228902] [-0.09357694]]
v["dW2"]	[[-0.02678881 0.05303555 -0.06916608] [-0.03967535 -0.06871727 -0.08452056] [-0.06712461 -0.00126646 -0.11173103]]
v["db2"]	[[ 0.02344157] [ 0.16598022] [ 0.07420442]]
s["dW1"]	[[ 0.00121136 0.00131039 0.00081287] [ 0.0002525 0.00081154 0.00046748]]
s["db1"]	[[ 1.51020075e-05] [ 8.75664434e-04]]
s["dW2"]	[[ 7.17640232e-05 2.81276921e-04 4.78394595e-04] [ 1.57413361e-04 4.72206320e-04 7.14372576e-04] [ 4.50571368e-04 1.60392066e-07 1.24838242e-03]]
s["db2"]	[[ 5.49507194e-05] [ 2.75494327e-03] [ 5.50629536e-04]]
#+end_example

** exploration
Lets examine how the different optimization algorithms compare.

#+begin_src python :results file :exports both
train_X, train_Y = load_dataset()
plt.savefig('moons-dataset.png')
plt.close()

'moons-dataset.png'
#+end_src

#+RESULTS:
[[file:moons-dataset.png]]

Our model is going to be a 3-layer neural net:

#+begin_src python :results silent
def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9, beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):
    """
    3-layer neural network model which can be run in different optimizer modes.
    
    Arguments:
    X -- input data, of shape (2, number of examples)
    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)
    layers_dims -- python list, containing the size of each layer
    learning_rate -- the learning rate, scalar.
    mini_batch_size -- the size of a mini batch
    beta -- Momentum hyperparameter
    beta1 -- Exponential decay hyperparameter for the past gradients estimates
    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates
    epsilon -- hyperparameter preventing division by zero in Adam updates
    num_epochs -- number of epochs
    print_cost -- True to print the cost every 1000 epochs

    Returns:
    parameters -- python dictionary containing your updated parameters
    """
    L = len(layers_dims)             # number of layers in the neural networks
    costs = []                       # to keep track of the cost
    t = 0                            # initializing the counter required for Adam update
    seed = 10                        # For grading purposes, so that your "random" minibatches are the same as ours
    
    # Initialize parameters
    parameters = initialize_parameters(layers_dims)
    
    # Initialize the optimizer
    if optimizer == "gd":
        pass # no initialization required for gradient descent
        
    elif optimizer == "momentum":
        v = initialize_velocity(parameters)
        
    elif optimizer == "adam":
        v, s = initialize_adam(parameters)
        
    # Optimization loop
    for i in range(num_epochs):
        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch
        seed = seed + 1
        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)
        
        for minibatch in minibatches:
            # Select a minibatch
            (minibatch_X, minibatch_Y) = minibatch
            
            # Forward propagation
            a3, caches = forward_propagation(minibatch_X, parameters)
            
            # Compute cost
            cost = compute_cost(a3, minibatch_Y)
            
            # Backward propagation
            grads = backward_propagation(minibatch_X, minibatch_Y, caches)
            
            # Update parameters
            if optimizer == "gd":
                parameters = update_parameters_with_gd(parameters, grads, learning_rate)
                
            elif optimizer == "momentum":
                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)
                
            elif optimizer == "adam":
                t = t + 1 # Adam counter
                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2,  epsilon)
                
        # Print the cost every 1000 epoch
        if print_cost and i % 1000 == 0:
            print ("Cost after epoch %i: %f" %(i, cost))
            
        if print_cost and i % 100 == 0:
            costs.append(cost)
            
    # plot the cost
    plt.plot(costs)
    plt.ylabel('cost')
    plt.xlabel('epochs (per 100)')
    plt.title("Learning rate = " + str(learning_rate))
    
    return parameters
#+end_src

*** mini-batch gradient descent
First, lets look at mini-batch gradient descent

#+begin_src python :results output :exports both
# train 3-layer model
layers_dims = [train_X.shape[0], 5, 2, 1]
parameters = model(train_X, train_Y, layers_dims, optimizer = "gd")
plt.savefig('mbgd-learning-rate.png')
plt.close()

# Predict
predictions = predict(train_X, train_Y, parameters)

# Plot decision boundary
plt.title("Model with Gradient Descent optimization")
axes = plt.gca()
axes.set_xlim([-1.5,2.5])
axes.set_ylim([-1,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
plt.savefig('mbgd-boundary.png')
plt.close()
#+end_src

#+RESULTS:
#+begin_example
Cost after epoch 0: 0.690736
Cost after epoch 1000: 0.685273
Cost after epoch 2000: 0.647072
Cost after epoch 3000: 0.619525
Cost after epoch 4000: 0.576584
Cost after epoch 5000: 0.607243
Cost after epoch 6000: 0.529403
Cost after epoch 7000: 0.460768
Cost after epoch 8000: 0.465586
Cost after epoch 9000: 0.464518
Accuracy: 0.7966666666666666
#+end_example

[[file:mbgd-learning-rate.png]]
[[file:mbgd-boundary.png]]

The oscillations in the costs if normal, since we're calculating the cost by
using only a subset of the examples (the mini-batch), but the trend should still
be downward, which is what we're getting.

*** MBGD with momentum
#+begin_src python :results output :exports both
# train 3-layer model
layers_dims = [train_X.shape[0], 5, 2, 1]
parameters = model(train_X, train_Y, layers_dims, beta = 0.9, optimizer = "momentum")
plt.savefig('momentum-learning-rate.png')
plt.close()

# Predict
predictions = predict(train_X, train_Y, parameters)

# Plot decision boundary
plt.title("Model with Momentum optimization")
axes = plt.gca()
axes.set_xlim([-1.5,2.5])
axes.set_ylim([-1,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
plt.savefig('momentum-boundary.png')
plt.close()
#+end_src

#+RESULTS:
#+begin_example
Cost after epoch 0: 0.690741
Cost after epoch 1000: 0.685341
Cost after epoch 2000: 0.647145
Cost after epoch 3000: 0.619594
Cost after epoch 4000: 0.576665
Cost after epoch 5000: 0.607324
Cost after epoch 6000: 0.529476
Cost after epoch 7000: 0.460936
Cost after epoch 8000: 0.465780
Cost after epoch 9000: 0.464740
Accuracy: 0.7966666666666666
#+end_example

[[file:momentum-learning-rate.png]]
[[file:momentum-boundary.png]]


Because the example is relatively simple, we don't see a lot of improvement
using momentum.

*** MBGD with Adam
#+begin_src python :results output :exports both
# train 3-layer model
layers_dims = [train_X.shape[0], 5, 2, 1]
parameters = model(train_X, train_Y, layers_dims, optimizer = "adam")
plt.savefig('adam-learning-rate.png')
plt.close()

# Predict
predictions = predict(train_X, train_Y, parameters)

# Plot decision boundary
plt.title("Model with Adam optimization")
axes = plt.gca()
axes.set_xlim([-1.5,2.5])
axes.set_ylim([-1,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
plt.savefig('adam-boundary.png')
plt.close()
#+end_src

#+RESULTS:
#+begin_example
Cost after epoch 0: 0.690552
Cost after epoch 1000: 0.185567
Cost after epoch 2000: 0.150852
Cost after epoch 3000: 0.074454
Cost after epoch 4000: 0.125936
Cost after epoch 5000: 0.104235
Cost after epoch 6000: 0.100552
Cost after epoch 7000: 0.031601
Cost after epoch 8000: 0.111709
Cost after epoch 9000: 0.197648
Accuracy: 0.94
#+end_example

[[file:adam-learning-rate.png]]
[[file:adam-boundary.png]]

Here we see a much bigger improvement, it turns out this is because adam
converges mush faster than the other two, if we let them run for longer, all
three models would end up with good results.

Some of the key advantages of Adam:
- low memory requirements
- works well with little tuning of hyperparameters
- works well on many types of problems

You can learn more by looking at the paper which introduced Adam:  https://arxiv.org/pdf/1412.6980.pdf
