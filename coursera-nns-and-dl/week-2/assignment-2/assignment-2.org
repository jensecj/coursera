#+OPTIONS: toc:nil html-postamble:nil
#+PROPERTY: header-args:python :session week2sess :tangle cat-predict.py :exports code

* assignment 2
Implement a simple model for predicting whether in image has a cat in it, or
not.

** setup
First, we need to import the libraries we'll be using.
#+begin_src python :results silent
import numpy as np
import matplotlib.pyplot as plt
import h5py
import scipy
from PIL import Image
from scipy import ndimage
#+end_src

The dataset we're going to use is a set of cat images, labled with '1' for
cat. each image has the shape (width, height, 3), where width = height (i.e. the
images are square), the 3 is for the RGB value at each pixel.
#+begin_src python :results silent
train_dataset = h5py.File('datasets/train_catvnoncat.h5', "r")
train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # your train set features
train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # your train set labels

test_dataset = h5py.File('datasets/test_catvnoncat.h5', "r")
test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # your test set features
test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # your test set labels

classes = np.array(test_dataset["list_classes"][:]) # the list of classes

train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))
#+end_src

Lets look at the shape of the dataset
#+begin_src python :results output :exports both
m_train = train_set_x_orig.shape[0]
m_test = test_set_x_orig.shape[0]
num_px = train_set_x_orig[0].shape[0]

print ("Number of training examples: m_train = " + str(m_train))
print ("Number of testing examples: m_test = " + str(m_test))
print ("Height/Width of each image: num_px = " + str(num_px))
print ("Each image is of size: (" + str(num_px) + ", " + str(num_px) + ", 3)")
print ("train_set_x shape: " + str(train_set_x_orig.shape))
print ("train_set_y shape: " + str(train_set_y_orig.shape))
print ("test_set_x shape: " + str(test_set_x_orig.shape))
print ("test_set_y shape: " + str(test_set_y_orig.shape))
#+end_src

#+RESULTS:
: Number of training examples: m_train = 209
: Number of testing examples: m_test = 50
: Height/Width of each image: num_px = 64
: Each image is of size: (64, 64, 3)
: train_set_x shape: (209, 64, 64, 3)
: train_set_y shape: (1, 209)
: test_set_x shape: (50, 64, 64, 3)
: test_set_y shape: (1, 50)

We do some preprocessing to the inputs to make them easier to work with.

We unroll the images into a single 1d vector:
#+begin_src python :results output :exports both
train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T
test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T

print ("train_set_x_flatten shape: " + str(train_set_x_flatten.shape))
print ("train_set_y shape: " + str(train_set_y_orig.shape))
print ("test_set_x_flatten shape: " + str(test_set_x_flatten.shape))
print ("test_set_y shape: " + str(test_set_y_orig.shape))
print ("sanity check after reshaping: " + str(train_set_x_flatten[0:5,0]))
#+end_src

#+RESULTS:
: train_set_x_flatten shape: (12288, 209)
: train_set_y shape: (1, 209)
: test_set_x_flatten shape: (12288, 50)
: test_set_y shape: (1, 50)
: sanity check after reshaping: [17 31 56 22 33]

Then we standardize the dataset by normalizing each example, since each example
is just an RGB value, we can divide with 255, which is the range of the RGB
color-space.

#+begin_src python :results silent
train_set_x = train_set_x_flatten/255.
test_set_x = test_set_x_flatten/255.
#+end_src

The common steps for preprocessing:
- Figure out dimensions and shapes of the problem (number of training / test
  examples, size of images, etc.)
- Reshape the datasets such that each example is a vector of a proper size
  (e.g. we created a $(width * height * 3, 1)$ vector)
- standardize the data

** the learning algorithm
[[file:images/LogReg_kiank.png]]

We have previously looked at the formulae used to training on a single example
of the dataset.

for an example $x_i$:
- calculating the estimate:
$z_i = w^Tx_i + b$ where $w,b$ are the parameters

- calculating the activation:
$\hat{y}_i = a_i = \sigma(z_i)$

- calculating the loss:
$L(\hat{y}, y) = -(y \log \hat{y} + (1 - y) \log(1-\hat{y}))$,
where y is the label for the example (expected output)

then we can calculating the cost over all training examples:
$J = \frac{1}{m} \sum_{i=1}^m{L(\hat{y}^i, y^i)}$

once we have the cost, we can figure out the gradients:

$\frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T$

$\frac{\partial J}{\partial b} = \frac{1}{m}\sum_{i = 1}^m(a_i-y_i)$

we can then use these gradients to update our parameters.

** implementation

We'll split the model up into distinct parts:
- initialization
- forwards and backwards propagation (calculate cost and gradient)
- optimization (learn parameters using gradient descent)

once this is done, we can use the learned parameters to make a prediction on a
new example.

we can use numpys ~np.zeros~ to initialize matrices with zeroes.
#+begin_src python :results silent
def initialize_with_zeros(dim):
    """
    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.
    
    Argument:
    dim -- size of the w vector we want (or number of parameters in this case)
    
    Returns:
    w -- initialized vector of shape (dim, 1)
    b -- initialized scalar (corresponds to the bias)
    """
    
    w = np.zeros((dim ,1))
    b = 0
    
    assert(w.shape == (dim, 1))
    assert(isinstance(b, float) or isinstance(b, int))
    
    return w, b
#+end_src

an example:
#+begin_src python :results output :exports both
dim = 2
w, b = initialize_with_zeros(dim)
print ("w = " + str(w))
print ("b = " + str(b))
#+end_src

#+RESULTS:
: w = [[0.]
:  [0.]]
: b = 0

To calculate the propagation, we'll first need the activation function, here
we'll use $sigmoid(z) = \frac{1}{1+e^{-z}}$.

#+begin_src python :results silent
def sigmoid(z):
    """
    Compute the sigmoid of z
    
    Arguments:
    z -- A scalar or numpy array of any size.
    
    Return:
    s -- sigmoid(z)
    """
    return 1/(1+np.exp(-z))
#+end_src

#+begin_src python :results output :exports both
print ("sigmoid([0, 2]) = " + str(sigmoid(np.array([0,2]))))
#+end_src

#+RESULTS:
: sigmoid([0, 2]) = [0.5        0.88079708]

expected output: sigmoid([0, 2]) = [ 0.5 0.88079708 ]

We'll do the forwards and backwards propagation in a single function:
#+begin_src python :results silent
def propagate(w, b, X, Y):
    """
    Implement the cost function and its gradient for the propagation explained above
    
    Arguments:
    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
    b -- bias, a scalar
    X -- data of size (num_px * num_px * 3, number of examples)
    Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)
    
    Return:
    cost -- negative log-likelihood cost for logistic regression
    dw -- gradient of the loss with respect to w, thus same shape as w
    db -- gradient of the loss with respect to b, thus same shape as b
    
    Tips:
    - Write your code step by step for the propagation. np.log(), np.dot()
    """
    m = X.shape[1]
    
    # forward propagation (from x to cost)
    A = sigmoid(np.dot(w.T, X) + b) # compute activations
    cost = -(1/m) * np.sum(Y*np.log(A)+(1-Y)*np.log(1-A)) # compute cost
    
    # backward propagation (to find gradient)
    dw = (1/m) * np.dot(X, (A - Y).T)
    db = (1/m) * np.sum(A - Y)
    
    assert(dw.shape == w.shape)
    assert(db.dtype == float)
    cost = np.squeeze(cost)
    assert(cost.shape == ())
    
    grads = {"dw": dw, "db": db}
    
    return grads, cost
#+end_src

#+begin_src python :results output :exports both
w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])
grads, cost = propagate(w, b, X, Y)
print ("dw = " + str(grads["dw"]))
print ("db = " + str(grads["db"]))
print ("cost = " + str(cost))
#+end_src

#+RESULTS:
: dw = [[0.99845601]
:  [2.39507239]]
: db = 0.001455578136784208
: cost = 5.801545319394553

expected output: 
dw:	[[ 0.99845601] [ 2.39507239]]
db:	0.00145557813678
cost:	5.801545319394553

Now we can use the computed costs and gradients to optimize the parameters using
gradient descent.

#+begin_src python :results silent
def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):
    """
    This function optimizes w and b by running a gradient descent algorithm
    
    Arguments:
    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
    b -- bias, a scalar
    X -- data of shape (num_px * num_px * 3, number of examples)
    Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)
    num_iterations -- number of iterations of the optimization loop
    learning_rate -- learning rate of the gradient descent update rule
    print_cost -- True to print the loss every 100 steps
    
    Returns:
    params -- dictionary containing the weights w and bias b
    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function
    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.
    
    Tips:
    You basically need to write down two steps and iterate through them:
        1) Calculate the cost and the gradient for the current parameters. Use propagate().
        2) Update the parameters using gradient descent rule for w and b.
    """
    costs = []
    
    for i in range(num_iterations):
        grads, cost = propagate(w,b,X,Y)
        
        dw = grads["dw"]
        db = grads["db"]
        
        # update parameters
        w = w - learning_rate * dw
        b = b - learning_rate * db
        
        # record the costs so we can plot how well we're doing
        if i % 100 == 0:
            costs.append(cost)
            
        # print the cost every 100 training iterations
        if print_cost and i % 100 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))
            
    params = {"w": w, "b": b}
    grads = {"dw": dw, "db": db}
    
    return params, grads, costs
#+end_src

#+begin_src python :results output :exports both
params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)

print ("w = " + str(params["w"]))
print ("b = " + str(params["b"]))
print ("dw = " + str(grads["dw"]))
print ("db = " + str(grads["db"]))
#+end_src

#+RESULTS:
: w = [[0.19033591]
:  [0.12259159]]
: b = 1.9253598300845747
: dw = [[0.67752042]
:  [1.41625495]]
: db = 0.21919450454067657


Expected Output:
w:	[[ 0.19033591] [ 0.12259159]]
b:	1.92535983008
dw:	[[ 0.67752042] [ 1.41625495]]
db:	0.219194504541

Now we have learned the parameters $w$ and $b$m lets try and use them to predict
a new example.

#+begin_src python :results silent
def predict(w, b, X):
    '''
    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)
    
    Arguments:
    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
    b -- bias, a scalar
    X -- data of size (num_px * num_px * 3, number of examples)
    
    Returns:
    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X
    '''
    m = X.shape[1]
    Y_prediction = np.zeros((1,m))
    w = w.reshape(X.shape[0], 1)
    
    # Compute vector "A" predicting the probabilities of a cat being present in the picture
    A = sigmoid(np.dot(w.T,X)+b)
    
    # you can also just use np.round(A) to round every number in A.
    for i in range(A.shape[1]):
        # Convert probabilities A[0,i] to actual predictions p[0,i]
        Y_prediction[0,i] = A[0,i] > 0.5
        
    assert(Y_prediction.shape == (1, m))
    
    return Y_prediction
#+end_src

#+begin_src python :results output :exports both
w = np.array([[0.1124579],[0.23106775]])
b = -0.3
X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])
print ("predictions = " + str(predict(w, b, X)))
#+end_src

#+RESULTS:
: predictions = [[1. 1. 0.]]

Expected Output: [ [ 1. 1. 0.] ]

Now we can merge all the disjunct functions we have created to create a model.

#+begin_src python :results silent
def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):
    """
    Builds the logistic regression model by calling the function you've implemented previously
    
    Arguments:
    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)
    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)
    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)
    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)
    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters
    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()
    print_cost -- Set to true to print the cost every 100 iterations
    
    Returns:
    d -- dictionary containing information about the model.
    """
    # initialize parameters with zeros
    m = X_train.shape[0]
    w, b = initialize_with_zeros(m)
    
    # Gradient descent
    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)
    
    # Retrieve parameters w and b from dictionary "parameters"
    w = parameters["w"]
    b = parameters["b"]
    
    # Predict test/train set examples
    Y_prediction_test = predict(w, b, X_test)
    Y_prediction_train = predict(w, b, X_train)
    
    # Print train/test Errors
    print("train accuracy: {} %".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))
    print("test accuracy: {} %".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))
    
    
    d = {"costs": costs,
         "Y_prediction_test": Y_prediction_test, 
         "Y_prediction_train" : Y_prediction_train, 
         "w" : w, 
         "b" : b,
         "learning_rate" : learning_rate,
         "num_iterations": num_iterations}
    
    return d
#+end_src

Using the model:
#+begin_src python :results output :exports both
d = model(train_set_x, train_set_y_orig, test_set_x, test_set_y_orig, num_iterations = 2000, learning_rate = 0.005, print_cost = True)
#+end_src

#+RESULTS:
#+begin_example
Cost after iteration 0: 0.693147
Cost after iteration 100: 0.584508
Cost after iteration 200: 0.466949
Cost after iteration 300: 0.376007
Cost after iteration 400: 0.331463
Cost after iteration 500: 0.303273
Cost after iteration 600: 0.279880
Cost after iteration 700: 0.260042
Cost after iteration 800: 0.242941
Cost after iteration 900: 0.228004
Cost after iteration 1000: 0.214820
Cost after iteration 1100: 0.203078
Cost after iteration 1200: 0.192544
Cost after iteration 1300: 0.183033
Cost after iteration 1400: 0.174399
Cost after iteration 1500: 0.166521
Cost after iteration 1600: 0.159305
Cost after iteration 1700: 0.152667
Cost after iteration 1800: 0.146542
Cost after iteration 1900: 0.140872
train accuracy: 99.04306220095694 %
test accuracy: 70.0 %
#+end_example

Expected Output:
Cost after iteration 0:	0.693147
...
Train Accuracy:	99.04306220095694 %
Test Accuracy:	70.0 %

We can see that the training accuracy is very high, so we know that the model
hash high enough capacity to fit the training data, but combined with the
accuracy for testing, we can probably say that the model is overfitting our
testing data quite a bit. 70% is pretty good for a simple model like this
though.

Now we can look at any index in our test set and see how well we did:
#+begin_src python :results output :exports both
index = 15
label = test_set_y_orig[0, index]
guess = int(d["Y_prediction_test"][0, index])
print ("y = " + str(label) + ", you predicted y =" + str(guess))
print("image of: " + classes[guess].decode("utf-8"))
#+end_src

#+RESULTS:
: y = 1, you predicted y =1
: image of: cat

We can also have a look at how the cost improves overtime:
#+begin_src python :results file :exports both
# Plot learning curve (with costs)
costs = np.squeeze(d['costs'])
plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title("Learning rate =" + str(d["learning_rate"]))
plt.savefig('costs_plot.png')
plt.close()

'costs_plot.png'
#+end_src

#+RESULTS:
[[file:costs_plot.png]]

Lets examine how changing the learning rate changes the cost-optimization:
#+begin_src python :results output :exports both
learning_rates = [0.01, 0.001, 0.0001]
models = {}
for i in learning_rates:
    print ("learning rate is: " + str(i))
    models[str(i)] = model(train_set_x, train_set_y_orig, test_set_x, test_set_y_orig, num_iterations = 1500, learning_rate = i, print_cost = False)
    print ('\n' + "-------------------------------------------------------" + '\n')

for i in learning_rates:
    plt.plot(np.squeeze(models[str(i)]["costs"]), label= str(models[str(i)]["learning_rate"]))

plt.ylabel('cost')
plt.xlabel('iterations (hundreds)')

legend = plt.legend(loc='upper center', shadow=True)
frame = legend.get_frame()
frame.set_facecolor('0.90')
plt.savefig('learning_rates.png')
plt.close()
#+end_src

#+RESULTS:
#+begin_example
learning rate is: 0.01
train accuracy: 99.52153110047847 %
test accuracy: 68.0 %

-------------------------------------------------------

learning rate is: 0.001
train accuracy: 88.99521531100478 %
test accuracy: 64.0 %

-------------------------------------------------------

learning rate is: 0.0001
train accuracy: 68.42105263157895 %
test accuracy: 36.0 %

-------------------------------------------------------
#+end_example

[[file:learning_rates.png]]

We can see that different learning rates give different costs and predictions.

In the example with the highest learning rate (which was also the rate we used),
the cost oscillates up and down. In some cases it may even diverge, though this
did not happen for us.

Low learning rates does not mean better models, the rate nees to be "just
right", otherwise we can run into over-fitting, slow learning, and other issues.

Finally, if we want to use our model to predict a new image, we need to load the
image, unroll its shape, and then we can feed it to our model:

#+begin_src python :results output :exports both
my_image = "images/my_image2.jpg"

# preprocess the image to fit your algorithm.
image = np.array(plt.imread(my_image))
my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T
my_predicted_image = predict(d["w"], d["b"], my_image)

prediction = int(np.squeeze(my_predicted_image))
print("you predict: y = " + str(prediction) + ", image is of " + classes[prediction,].decode("utf-8"))
#+end_src

#+RESULTS:
: you predict: y = 1, image is of cat
