#+OPTIONS: toc:nil html-postamble:nil
#+PROPERTY: header-args:python :session week-3-sess :tangle planar-nn.py :exports code

* week 3 assignment 1
** setup
import the packages we need
#+begin_src python :results silent
import numpy as np
import matplotlib.pyplot as plt
import sklearn
import sklearn.datasets
import sklearn.linear_model

np.random.seed(1) # set a seed so that the results are consistent
#+end_src

*** helper functions
#+begin_src python :results silent
def load_planar_dataset():
    np.random.seed(1)
    m = 400 # number of examples
    N = int(m/2) # number of points per class
    D = 2 # dimensionality
    X = np.zeros((m,D)) # data matrix where each row is a single example
    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)
    a = 4 # maximum ray of the flower
    
    for j in range(2):
        ix = range(N*j,N*(j+1))
        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta
        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius
        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]
        Y[ix] = j
        
    X = X.T
    Y = Y.T
    
    return X, Y
#+end_src

#+begin_src python :results silent
def plot_decision_boundary(model, X, y):
    # Set min and max values and give it some padding
    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1
    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predict the function value for the whole grid
    Z = model(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.ylabel('x2')
    plt.xlabel('x1')
    plt.scatter(X[0, :], X[1, :], c=y.ravel().tolist(), cmap=plt.cm.Spectral)
#+end_src

#+begin_src python :results silent
def sigmoid(x):
    """
    Compute the sigmoid of x

    Arguments:
    x -- A scalar or numpy array of any size.

    Return:
    s -- sigmoid(x)
    """
    s = 1/(1+np.exp(-x))
    return s
#+end_src

#+begin_src python :results silent
def load_extra_datasets():
    N = 200
    noisy_circles = sklearn.datasets.make_circles(n_samples=N, factor=.5, noise=.3)
    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=.2)
    blobs = sklearn.datasets.make_blobs(n_samples=N, random_state=5, n_features=2, centers=6)
    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(mean=None, cov=0.5, n_samples=N, n_features=2, n_classes=2, shuffle=True, random_state=None)
    no_structure = np.random.rand(N, 2), np.random.rand(N, 2)
    
    return noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure
#+end_src

** the dataset
use the helper function to load the "flower" dataset into memory.
#+begin_src python :results value
X, Y = load_planar_dataset()
#+end_src

#+RESULTS:

lets visualize the data:
#+begin_src python :results file :exports both
plt.scatter(X[0, :], X[1, :], c=Y.ravel().tolist(), s=50, cmap=plt.cm.Spectral);
plt.savefig('flower-data.png')
plt.close()

'flower-data.png'
#+end_src

#+RESULTS:
[[file:flower-data.png]]

and look at the shape of it:
#+begin_src python :results output :exports both
### START CODE HERE ### (â‰ˆ 3 lines of code)
shape_X = X.shape
shape_Y = Y.shape
m = X.shape[1]  # training set size
### END CODE HERE ###

print ('The shape of X is: ' + str(shape_X))
print ('The shape of Y is: ' + str(shape_Y))
print ('I have m = %d training examples!' % (m))
#+end_src

#+RESULTS:
: The shape of X is: (2, 400)
: The shape of Y is: (1, 400)
: I have m = 400 training examples!

#+begin_example
Expected Output:
shape of X	(2, 400)
shape of Y	(1, 400)
m =	400
#+end_example

** testing with logistic regression
We can try to first run some simple logistic regression on this data, like the
regression we did in week-2. Here we're using the built-in functions from sklearn.

#+begin_src python :results silent
# Train the logistic regression classifier
clf = sklearn.linear_model.LogisticRegressionCV();
clf.fit(X.T, Y.T);
#+end_src

Lets look a how well logistic regression does on the dataset.
#+begin_src python :results file :exports both
plot_decision_boundary(lambda x: clf.predict(x), X, Y)
plt.title("Logistic Regression")
plt.savefig('logistic-regression-test.png')
plt.close()

'logistic-regression-test.png'
#+end_src

#+RESULTS:
[[file:logistic-regression-test.png]]

and check how accurate the model was.
#+begin_src python :results output :exports both
LR_predictions = clf.predict(X.T)
prediction_accuracy = float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions)) / float(Y.size) * 100)
print ('Accuracy of logistic regression: %d ' % prediction_accuracy
+ '% ' + "(percentage of correctly labelled datapoints)")
#+end_src

#+RESULTS:
: Accuracy of logistic regression: 47 % (percentage of correctly labelled datapoints)

So we can see that logistic regression does not do very well on data that is not
linearly-separable.


** neural networks
The simple model we will implement:
[[file:images/classification_kiank.png]]

we have two features, a hidden layer of 4 units, each of which uses $tanh$ as
its activation function, and a single output unit using the $sigmoid$ activation
function, because we're trying to do binary classification (between the red and
blue points in the dataset).

The methodology we will follow to build the network is:
1. define the networks structure (number of inputs, layers, hidden units, etc.)
2. initialize the models parameters
3. iterate until error is low:
   - forward propagation, to calculate activations
   - compute loss
   - backward propagation, to calculate gradients
   - update parameters, using gradient descent

here we'll use the three variables $n_x, n_h, n_y$ to describe the number of
units in the input, hidden, and output layers respectively.

We create a simple function to generate the layer sizes we need for out model
given the input data.
#+begin_src python :results silent
def layer_sizes(X, Y):
    """
    Arguments:
    X -- input dataset of shape (input size, number of examples)
    Y -- labels of shape (output size, number of examples)
    
    Returns:
    n_x -- the size of the input layer
    n_h -- the size of the hidden layer
    n_y -- the size of the output layer
    """
    n_x = X.shape[0] # size of input layer
    n_h = 4
    n_y = Y.shape[0] # size of output layer
    return (n_x, n_h, n_y)
#+end_src

Let's test it:
#+begin_src python :results silent
def layer_sizes_test_case():
    np.random.seed(1)
    X_assess = np.random.randn(5, 3)
    Y_assess = np.random.randn(2, 3)
    return X_assess, Y_assess
#+end_src

#+begin_src python :results output :exports both
X_assess, Y_assess = layer_sizes_test_case()
(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)
print("The size of the input layer is: n_x = " + str(n_x))
print("The size of the hidden layer is: n_h = " + str(n_h))
print("The size of the output layer is: n_y = " + str(n_y))
#+end_src

#+RESULTS:
: The size of the input layer is: n_x = 5
: The size of the hidden layer is: n_h = 4
: The size of the output layer is: n_y = 2

#+begin_example
Expected Output
n_x =	5
n_h =	4
n_y =	2
#+end_example

Next we need to initialize the models parameters.
As seen in the weeks notes, we would like to initialize each units weights to
small random values, to break symmetry, and we can just initialize the bias
terms to 0.

#+begin_src python :results silent
def initialize_parameters(n_x, n_h, n_y):
    """
    Argument:
    n_x -- size of the input layer
    n_h -- size of the hidden layer
    n_y -- size of the output layer
    
    Returns:
    params -- python dictionary containing your parameters:
                    W1 -- weight matrix of shape (n_h, n_x)
                    b1 -- bias vector of shape (n_h, 1)
                    W2 -- weight matrix of shape (n_y, n_h)
                    b2 -- bias vector of shape (n_y, 1)
    """
    
    np.random.seed(2) # just for testing, so we can compare outputs
    
    W1 = np.random.randn(n_h, n_x) * 0.01
    b1 = np.zeros((n_h,1))
    W2 = np.random.randn(n_y, n_h) * 0.01
    b2 = np.zeros((n_y,1))
    
    assert (W1.shape == (n_h, n_x))
    assert (b1.shape == (n_h, 1))
    assert (W2.shape == (n_y, n_h))
    assert (b2.shape == (n_y, 1))
    
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    
    return parameters
#+end_src

#+begin_src python :results silent
def initialize_parameters_test_case():
    n_x, n_h, n_y = 2, 4, 1
    return n_x, n_h, n_y
#+end_src

#+begin_src python :results output :exports both
n_x, n_h, n_y = initialize_parameters_test_case()

parameters = initialize_parameters(n_x, n_h, n_y)
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
#+end_src

#+RESULTS:
#+begin_example
W1 = [[-0.00416758 -0.00056267]
 [-0.02136196  0.01640271]
 [-0.01793436 -0.00841747]
 [ 0.00502881 -0.01245288]]
b1 = [[0.]
 [0.]
 [0.]
 [0.]]
W2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]
b2 = [[0.]]
#+end_example

#+begin_example
Expected Output:

W1 = [[-0.00416758 -0.00056267] [-0.02136196 0.01640271] [-0.01793436 -0.00841747] [ 0.00502881 -0.01245288]]
b1 = [[ 0.] [ 0.] [ 0.] [ 0.]]
W2 = [ [-0.01057952 -0.00909008 0.00551454 0.02292208] ]
b2 = [ [ 0.] ]
#+end_example

Next up is forward propagation, the vectorized implementation is fairly simple,
and follows easily from the image of the model from earlier.
#+begin_src python :results silent
def forward_propagation(X, parameters):
    """
    Argument:
    X -- input data of size (n_x, m)
    parameters -- python dictionary containing your parameters (output of initialization function)
    
    Returns:
    A2 -- The sigmoid output of the second activation
    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"
    """
    # Retrieve each parameter from the dictionary "parameters"
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    
    # Implement Forward Propagation to calculate A2 (probabilities)
    Z1 = np.dot(W1, X)+b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2, A1)+b2
    A2 = sigmoid(Z2)
    
    assert(A2.shape == (1, X.shape[1]))
    
    cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}
    
    return A2, cache
#+end_src

#+begin_src python :results silent
def forward_propagation_test_case():
    np.random.seed(1)
    X_assess = np.random.randn(2, 3)
    b1 = np.random.randn(4,1)
    b2 = np.array([[ -1.3]])
    
    parameters = {'W1': np.array([[-0.00416758, -0.00056267],
        [-0.02136196,  0.01640271],
        [-0.01793436, -0.00841747],
        [ 0.00502881, -0.01245288]]),
     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),
     'b1': b1,
     'b2': b2}
    
    return X_assess, parameters
#+end_src

#+begin_src python :results output :exports both
X_assess, parameters = forward_propagation_test_case()

A2, cache = forward_propagation(X_assess, parameters)

# Note: we use the mean here just to make sure that the output is easy to compare
print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))
#+end_src

#+RESULTS:
: 0.26281864019752443 0.09199904522700113 -1.3076660128732143 0.21287768171914198

#+begin_example
Expected Output:
0.262818640198 0.091999045227 -1.30766601287 0.212877681719
#+end_example

Onto the cost function.
Cross-entropy loss, as described in the lectures, is defined as
$$
J = - \frac{1}{m} \sum_{i=0}^m (y^{(i)} \log(a^{[2](i)}) + (1 - y^{(i)})
\log(1 - a^{[2](i)}))
$$

since were doing a vectorized implementation, the implementation looks a lot
less scary:

#+begin_src python :results silent
def compute_cost(A2, Y, parameters):
    """
    Computes the cross-entropy cost given in equation (13)
    
    Arguments:
    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)
    Y -- "true" labels vector of shape (1, number of examples)
    parameters -- python dictionary containing your parameters W1, b1, W2 and b2
    
    Returns:
    cost -- cross-entropy cost given equation (13)
    """
    
    m = Y.shape[1] # number of example
    
    # Compute the cross-entropy cost
    logprobs = np.multiply(Y, np.log(A2)) + np.multiply((1-Y), np.log(1 - A2))
    cost = np.multiply(-(1/m), np.sum(logprobs))
    
    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. E.g., turns [[17]] into 17
    assert(isinstance(cost, float))
    
    return cost
#+end_src

#+begin_src python :results silent
def compute_cost_test_case():
    np.random.seed(1)
    Y_assess = (np.random.randn(1, 3) > 0)
    parameters = {'W1': np.array([[-0.00416758, -0.00056267],
        [-0.02136196,  0.01640271],
        [-0.01793436, -0.00841747],
        [ 0.00502881, -0.01245288]]),
     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),
     'b1': np.array([[ 0.],
        [ 0.],
        [ 0.],
        [ 0.]]),
     'b2': np.array([[ 0.]])}
    
    a2 = (np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]))
    
    return a2, Y_assess, parameters
#+end_src

#+begin_src python :results output :exports both
A2, Y_assess, parameters = compute_cost_test_case()

print("cost = " + str(compute_cost(A2, Y_assess, parameters)))
#+end_src

#+RESULTS:
: cost = 0.6930587610394646

#+begin_example
Expected Output:
cost	0.693058761...
#+end_example

We have now calculated the activations using forward propagation, and calculated
the loss of those, now we need to calculate the gradients using backward
propagation.

[[file:images/grad_summary.png]]

Following the formulae on the right-hard side is almost formulaic. the one this
we need to note here is that we need the derivative of the activation function
($g'$), for $tanh$, this turns out to be $g^{[1]}'(z) = (1 - a^2)$,
where $a = g^{[1]}(z)$.

#+begin_src python :results silent
def backward_propagation(parameters, cache, X, Y):
    """
    Implement the backward propagation using the instructions above.
    
    Arguments:
    parameters -- python dictionary containing our parameters
    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".
    X -- input data of shape (2, number of examples)
    Y -- "true" labels vector of shape (1, number of examples)
    
    Returns:
    grads -- python dictionary containing your gradients with respect to different parameters
    """
    m = X.shape[1]
    
    # First, retrieve W1 and W2 from the dictionary "parameters".
    W1 = parameters["W1"]
    W2 = parameters["W2"]
        
    # Retrieve also A1 and A2 from dictionary "cache".
    A1 = cache["A1"]
    A2 = cache["A2"]
    
    # Backward propagation: calculate dW1, db1, dW2, db2.
    dZ2 = A2 - Y
    dW2 = 1/m * np.dot(dZ2, A1.T)
    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)
    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))
    dW1 = 1/m * np.dot(dZ1, X.T)
    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)
    
    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}
    
    return grads
#+end_src

#+begin_src python :results silent
def backward_propagation_test_case():
    np.random.seed(1)
    X_assess = np.random.randn(2, 3)
    Y_assess = (np.random.randn(1, 3) > 0)
    parameters = {'W1': np.array([[-0.00416758, -0.00056267],
        [-0.02136196,  0.01640271],
        [-0.01793436, -0.00841747],
        [ 0.00502881, -0.01245288]]),
     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),
     'b1': np.array([[ 0.],
        [ 0.],
        [ 0.],
        [ 0.]]),
     'b2': np.array([[ 0.]])}
    
    cache = {'A1': np.array([[-0.00616578,  0.0020626 ,  0.00349619],
         [-0.05225116,  0.02725659, -0.02646251],
         [-0.02009721,  0.0036869 ,  0.02883756],
         [ 0.02152675, -0.01385234,  0.02599885]]),
  'A2': np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]),
  'Z1': np.array([[-0.00616586,  0.0020626 ,  0.0034962 ],
         [-0.05229879,  0.02726335, -0.02646869],
         [-0.02009991,  0.00368692,  0.02884556],
         [ 0.02153007, -0.01385322,  0.02600471]]),
  'Z2': np.array([[ 0.00092281, -0.00056678,  0.00095853]])}
    return parameters, cache, X_assess, Y_assess
#+end_src

#+begin_src python :results output :exports both
parameters, cache, X_assess, Y_assess = backward_propagation_test_case()

grads = backward_propagation(parameters, cache, X_assess, Y_assess)
print ("dW1 = "+ str(grads["dW1"]))
print ("db1 = "+ str(grads["db1"]))
print ("dW2 = "+ str(grads["dW2"]))
print ("db2 = "+ str(grads["db2"]))
#+end_src

#+RESULTS:
#+begin_example
dW1 = [[ 0.00301023 -0.00747267]
 [ 0.00257968 -0.00641288]
 [-0.00156892  0.003893  ]
 [-0.00652037  0.01618243]]
db1 = [[ 0.00176201]
 [ 0.00150995]
 [-0.00091736]
 [-0.00381422]]
dW2 = [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]]
db2 = [[-0.16655712]]
#+end_example

#+begin_example
Expected output:

dW1	[[ 0.00301023 -0.00747267] [ 0.00257968 -0.00641288] [-0.00156892 0.003893 ] [-0.00652037 0.01618243]]
db1	[[ 0.00176201] [ 0.00150995] [-0.00091736] [-0.00381422]]
dW2	[[ 0.00078841 0.01765429 -0.00084166 -0.01022527]]
db2	[[-0.16655712]]
#+end_example


Now all we have left to do is update the parameters using the calculated
gradients:

$\theta = \theta - \alpha \frac{\partial J}{\partial \theta}$, where $\alpha$ is the learning rate, and $\theta$ is some parameter.

We need to keep in mind the learning rate needs to be a proper size, otherwise
we may end up diverging while learning.

[[file:images/sgd.gif]]
[[file:images/sgd_bad.gif]]


#+begin_src python :results silent
def update_parameters(parameters, grads, learning_rate = 1.2):
    """
    Updates parameters using the gradient descent update rule given above
    
    Arguments:
    parameters -- python dictionary containing your parameters
    grads -- python dictionary containing your gradients
    
    Returns:
    parameters -- python dictionary containing your updated parameters
    """
    # Retrieve each parameter from the dictionary "parameters"
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    
    # Retrieve each gradient from the dictionary "grads"
    dW1 = grads["dW1"]
    db1 = grads["db1"]
    dW2 = grads["dW2"]
    db2 = grads["db2"]
    
    # Update rule for each parameter
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    
    return parameters
#+end_src

#+begin_src python :results silent
def update_parameters_test_case():
    parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],
        [-0.02311792,  0.03137121],
        [-0.0169217 , -0.01752545],
        [ 0.00935436, -0.05018221]]),
 'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),
 'b1': np.array([[ -8.97523455e-07],
        [  8.15562092e-06],
        [  6.04810633e-07],
        [ -2.54560700e-06]]),
 'b2': np.array([[  9.14954378e-05]])}
    
    grads = {'dW1': np.array([[ 0.00023322, -0.00205423],
        [ 0.00082222, -0.00700776],
        [-0.00031831,  0.0028636 ],
        [-0.00092857,  0.00809933]]),
 'dW2': np.array([[ -1.75740039e-05,   3.70231337e-03,  -1.25683095e-03,
          -2.55715317e-03]]),
 'db1': np.array([[  1.05570087e-07],
        [ -3.81814487e-06],
        [ -1.90155145e-07],
        [  5.46467802e-07]]),
 'db2': np.array([[ -1.08923140e-05]])}
    return parameters, grads
#+end_src

#+begin_src python :results output :exports both
parameters, grads = update_parameters_test_case()
parameters = update_parameters(parameters, grads)

print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
#+end_src

#+RESULTS:
#+begin_example
W1 = [[-0.00643025  0.01936718]
 [-0.02410458  0.03978052]
 [-0.01653973 -0.02096177]
 [ 0.01046864 -0.05990141]]
b1 = [[-1.02420756e-06]
 [ 1.27373948e-05]
 [ 8.32996807e-07]
 [-3.20136836e-06]]
W2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]
b2 = [[0.00010457]]
#+end_example

#+begin_example
Expected Output:

W1	[[-0.00643025 0.01936718] [-0.02410458 0.03978052] [-0.01653973 -0.02096177] [ 0.01046864 -0.05990141]]
b1	[[ -1.02420756e-06] [ 1.27373948e-05] [ 8.32996807e-07] [ -3.20136836e-06]]
W2	[[-0.01041081 -0.04463285 0.01758031 0.04747113]]
b2	[[ 0.00010457]]
#+end_example

Now we just need to combine all these functions to create our model:

#+begin_src python :results silent
def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):
    """
    Arguments:
    X -- dataset of shape (2, number of examples)
    Y -- labels of shape (1, number of examples)
    n_h -- size of the hidden layer
    num_iterations -- Number of iterations in gradient descent loop
    print_cost -- if True, print the cost every 1000 iterations
    
    Returns:
    parameters -- parameters learnt by the model. They can then be used to predict.
    """
    
    np.random.seed(3)
    n_x = layer_sizes(X, Y)[0]
    n_y = layer_sizes(X, Y)[2]
    
    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: "n_x, n_h, n_y". Outputs = "W1, b1, W2, b2, parameters".
    parameters = initialize_parameters(n_x, n_h, n_y)
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    
    # Loop (gradient descent)
    for i in range(0, num_iterations):
        # Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache".
        A2, cache = forward_propagation(X, parameters)
        
        # Cost function. Inputs: "A2, Y, parameters". Outputs: "cost".
        cost = compute_cost(A2, Y, parameters)
 
        # Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads".
        grads = backward_propagation(parameters, cache, X, Y)
 
        # Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".
        parameters = update_parameters(parameters, grads)
        
        # Print the cost every 1000 iterations
        if print_cost and i % 1000 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))
            
    return parameters
#+end_src

#+begin_src python :results silent
def nn_model_test_case():
    np.random.seed(1)
    X_assess = np.random.randn(2, 3)
    Y_assess = (np.random.randn(1, 3) > 0)
    return X_assess, Y_assess
#+end_src

#+begin_src python :results output :exports both
X_assess, Y_assess = nn_model_test_case()
parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=True)
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
#+end_src

#+RESULTS:
#+begin_example
Cost after iteration 0: 0.692739
Cost after iteration 1000: 0.000218
Cost after iteration 2000: 0.000107
Cost after iteration 3000: 0.000071
Cost after iteration 4000: 0.000053
Cost after iteration 5000: 0.000042
Cost after iteration 6000: 0.000035
Cost after iteration 7000: 0.000030
Cost after iteration 8000: 0.000026
Cost after iteration 9000: 0.000023
W1 = [[-0.65848169  1.21866811]
 [-0.76204273  1.39377573]
 [ 0.5792005  -1.10397703]
 [ 0.76773391 -1.41477129]]
b1 = [[ 0.287592  ]
 [ 0.3511264 ]
 [-0.2431246 ]
 [-0.35772805]]
W2 = [[-2.45566237 -3.27042274  2.00784958  3.36773273]]
b2 = [[0.20459656]]
#+end_example

#+begin_example
Expected Output:

cost after iteration 0	0.692739
â‹®â‹®
â‹®â‹®
W1	[[-0.65848169 1.21866811] [-0.76204273 1.39377573] [ 0.5792005 -1.10397703] [ 0.76773391 -1.41477129]]
b1	[[ 0.287592 ] [ 0.3511264 ] [-0.2431246 ] [-0.35772805]]
W2	[[-2.45566237 -3.27042274 2.00784958 3.36773273]]
b2	[[ 0.20459656]]
#+end_example

Now we have a trained model, we can now predict on some new data. a class is
predicted based on the activation of the output layer of out model, so we use
forward propagation to get that, then we predict 0 if $prediction < 0.5$, and 1
otherwise.

#+begin_src python :results silent
def predict(parameters, X):
    """
    Using the learned parameters, predicts a class for each example in X
    
    Arguments:
    parameters -- python dictionary containing your parameters
    X -- input data of size (n_x, m)
    
    Returns
    predictions -- vector of predictions of our model (red: 0 / blue: 1)
    """
    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.
    A2, cache = forward_propagation(X, parameters)
    predictions = np.round(A2)
    return predictions
#+end_src

#+begin_src python :results silent
def predict_test_case():
    np.random.seed(1)
    X_assess = np.random.randn(2, 3)
    parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],
        [-0.02311792,  0.03137121],
        [-0.0169217 , -0.01752545],
        [ 0.00935436, -0.05018221]]),
     'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),
     'b1': np.array([[ -8.97523455e-07],
        [  8.15562092e-06],
        [  6.04810633e-07],
        [ -2.54560700e-06]]),
     'b2': np.array([[  9.14954378e-05]])}
    
    return parameters, X_assess
#+end_src

#+begin_src python :results output :exports both
parameters, X_assess = predict_test_case()

predictions = predict(parameters, X_assess)
print("predictions mean = " + str(np.mean(predictions)))
#+end_src

#+RESULTS:
: predictions mean = 0.6666666666666666

#+begin_example
Expected Output:

predictions mean	0.666666666667
#+end_example

Lets try out or model on the planar dataset, and see how well it does.

#+begin_src python :results file :exports both
# Build a model with a n_h-dimensional hidden layer
parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)

# Plot the decision boundary
plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
plt.title("Decision Boundary for hidden layer size " + str(4))
plt.savefig('planar-predict.png')
plt.close()

'planar-predict.png'
#+end_src

#+RESULTS:
[[file:planar-predict.png]]

Much better than linear regression! it looks like the neural network has learned
the pattern of the "flower" pretty well.

#+begin_src python :results output :exports both
# Print accuracy
predictions = predict(parameters, X)
prediction_accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)
print ('Accuracy: %d' % prediction_accuracy + '%')
#+end_src

#+RESULTS:
: Accuracy: 90%


Let's experiment with how the number of hidden units change how the network
learns.

#+begin_src python :results file :exports both
plt.figure(figsize=(16, 32))
hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]
for i, n_h in enumerate(hidden_layer_sizes):
    plt.subplot(5, 2, i+1)
    plt.title('Hidden Layer of size %d' % n_h)
    parameters = nn_model(X, Y, n_h, num_iterations = 5000)
    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
    predictions = predict(parameters, X)
    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)
    print ("Accuracy for {} hidden units: {} %".format(n_h, accuracy))

plt.savefig('units-experiment.png')
plt.close()

'units-experiment.png'
#+end_src

#+RESULTS:
[[file:units-experiment.png]]


Looks like 4-5 units fits the data fairly well, much more than that makes it
overfit.

** other datasets
Lets look at how our model handles some other datasets!

#+begin_src python :results silent
noisy_circles, noisy_moons, blobs, gaussian_quantiles, _ = load_extra_datasets()
#+end_src

#+begin_src python :results file :exports both
X, Y = noisy_circles
X, Y = X.T, Y.reshape(1, Y.shape[0])

plt.scatter(X[0, :], X[1, :], c=Y.ravel().tolist(), s=40, cmap=plt.cm.Spectral);
plt.savefig('noisy_circles.png')
plt.close()
'noisy-circles.png'
#+end_src

#+RESULTS:
[[file:noisy_circles.png]]

#+begin_src python :results file :exports both
plt.figure(figsize=(16, 32))
hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]
for i, n_h in enumerate(hidden_layer_sizes):
    plt.subplot(5, 2, i+1)
    plt.title('Hidden Layer of size %d' % n_h)
    parameters = nn_model(X, Y, n_h, num_iterations = 5000)
    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
    predictions = predict(parameters, X)
    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)
    print ("Accuracy for {} hidden units: {} %".format(n_h, accuracy))

plt.savefig('noisy-circles-experiment.png')
plt.close()

'noisy-circles-experiment.png'
#+end_src

#+RESULTS:
[[file:noisy-circles-experiment.png]]


#+begin_src python :results file :exports both
X, Y = noisy_moons
X, Y = X.T, Y.reshape(1, Y.shape[0])

plt.scatter(X[0, :], X[1, :], c=Y.ravel().tolist(), s=40, cmap=plt.cm.Spectral);
plt.savefig('noisy_moons.png')
plt.close()
'noisy_moons.png'
#+end_src

#+RESULTS:
[[file:noisy_moons.png]]

#+begin_src python :results file :exports both
plt.figure(figsize=(16, 32))
hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]
for i, n_h in enumerate(hidden_layer_sizes):
    plt.subplot(5, 2, i+1)
    plt.title('Hidden Layer of size %d' % n_h)
    parameters = nn_model(X, Y, n_h, num_iterations = 5000)
    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
    predictions = predict(parameters, X)
    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)
    print ("Accuracy for {} hidden units: {} %".format(n_h, accuracy))

plt.savefig('noisy-moons-experiment.png')
plt.close()

'noisy-moons-experiment.png'
#+end_src

#+RESULTS:
[[file:noisy-moons-experiment.png]]


#+begin_src python :results file :exports both
X, Y = blobs
X, Y = X.T, Y.reshape(1, Y.shape[0])

Y = Y%2 # blobs are binary

plt.scatter(X[0, :], X[1, :], c=Y.ravel().tolist(), s=40, cmap=plt.cm.Spectral);
plt.savefig('blobs.png')
plt.close()
'blobs.png'
#+end_src

#+RESULTS:
[[file:blobs.png]]

#+begin_src python :results file :exports both
plt.figure(figsize=(16, 32))
hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]
for i, n_h in enumerate(hidden_layer_sizes):
    plt.subplot(5, 2, i+1)
    plt.title('Hidden Layer of size %d' % n_h)
    parameters = nn_model(X, Y, n_h, num_iterations = 5000)
    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
    predictions = predict(parameters, X)
    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)
    print ("Accuracy for {} hidden units: {} %".format(n_h, accuracy))

plt.savefig('blobs-experiment.png')
plt.close()

'blobs-experiment.png'
#+end_src

#+RESULTS:
[[file:blobs-experiment.png]]



#+begin_src python :results file :exports both
X, Y = gaussian_quantiles
X, Y = X.T, Y.reshape(1, Y.shape[0])

plt.scatter(X[0, :], X[1, :], c=Y.ravel().tolist(), s=40, cmap=plt.cm.Spectral);
plt.savefig('gaussian-quantiles.png')
plt.close()
'gaussian-quantiles.png'
#+end_src

#+RESULTS:
[[file:gaussian_quantiles.png]]

#+begin_src python :results file :exports both
plt.figure(figsize=(16, 32))
hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]
for i, n_h in enumerate(hidden_layer_sizes):
    plt.subplot(5, 2, i+1)
    plt.title('Hidden Layer of size %d' % n_h)
    parameters = nn_model(X, Y, n_h, num_iterations = 5000)
    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
    predictions = predict(parameters, X)
    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)
    print ("Accuracy for {} hidden units: {} %".format(n_h, accuracy))

plt.savefig('gaussian-quantiles-experiment.png')
plt.close()

'gaussian-quantiles-experiment.png'
#+end_src

#+RESULTS:
[[file:gaussian-quantiles-experiment.png]]


pretty well! Overfitting still turns out to be a problem for many hidden units,
but we'll handle that later.
